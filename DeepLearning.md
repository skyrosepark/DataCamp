
딥러닝의 사고나 기법은 새로운 것이 아닙니다. 그럼 왜 최근 몇 년동안 급격하게 딥러닝이 주목받게 된 것일까요?

#### 딥러닝의 과제인 '과적합'(over-fitting)의 회피
'구글의 고양이', 'DQN', '알파고'와 세계적인 이미지 대회인 'ILSVRC'에서도 딥러닝이 좋은 성적을 거뒀습니다. 이 정도의 성과를 올리기 시작한 배경에는 돌파구라고 할 만한 기술적인 진전이 있었습니다. 컴퓨터ㅣ 성능이 향상됐고 빅데이터를 이용할 수 있게 됐으며, 또 딥러닝 특유의 과제인 '과적합'을 회피할 수 있었던 것입니다
심층 신경망(Deep Learning)은 중간층을 다층화시켜 더욱 깊게 사고할 수 있습니다. 다층이 될수록 뉴런 처리와 전달, 산출되는 특징값이 늘어나며 이에 따라 답의 정확도가 올라갈 것이라는 사실은 이전부터 연구된 것입니다
'과적합'(over-fitting)이라고 불리는 문제는 심층 신경망을 마구잡이로 다층화해 파라미터의 수가 너무 많아질 때 발생하기 쉽다고 합니다. 과적합의 영향으로 인한 악영향으로 낯익은 훈련 데이터에는 정확도가 높은 답을 할 수 있는 한편, 훈련 데이터가 없는 미지의 데이터인 경우 정밀도가 내려가는(훈련 데이터의 영향을 너무 많이 받음) 현상이 있습니다. 즉, 훈련할 때는 성적이 좋았는데 실전에 가서는 성과를 내지 못하는 상태를 말합니다. 이것은 범용성이 부족하다는 과제를 낳았고 딥러닝에게는 정체기라고도 할 수 있는 시간을 불러왔습니다

#### 합성곱 신경망(CNN, Convolution Neural Network)
이 과제의 구체적인 해결책이 바로 '합성곱 신경망' 또는 '컨볼루션 신경망'(CNN)입니다
뉴런이 많고 복잡해지면 원래는 아무 관계도 없던 결합이 늘어나고 그것이 결국 악영향을 미쳐 '과적합'의 원인을 만들기도 합니다. 기계 정답률을 올리려면 층을 늘리는 한편 '아무 관계도 없는 결합을 끊어내는' 것이 중요합니다
또한 '오차역전파법'(Backpropagation)에 의해 출력 쪽에서 오차를 확인하게 하는 방법도 소개한 바 있는데, 모두 결합한 후 다층으로 만든 상태에서 오차역전파법을 써버리면 오차 전파가 분산되어 전혀 학습이 진전되지 않기 때문에 그 사태를 피하기 위해서는 아무 관계도 없는 결합을 잘라 버려야 한다는 이론도 있습니다
